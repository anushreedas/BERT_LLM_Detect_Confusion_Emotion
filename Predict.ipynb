{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict labels for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import numpy as np\n",
    "from transformers import *\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name,hidden_layers, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 4\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        bert_base_cased = BertModel.from_pretrained(model_name)  # Instantiate model using the trained weights\n",
    "        config = BertConfig.from_pretrained(model_name)\n",
    "        if hidden_layers != 12:\n",
    "            config.num_hidden_layers = hidden_layers\n",
    "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# create class for dataset loader\n",
    "class bertDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# load test dataset\n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        data = np.array(list(reader))\n",
    "        return data\n",
    "\n",
    "# load bert model with given number of hidden layers\n",
    "def load_model(model_path, num_hidden_layer):\n",
    "    print(\"Loading Bert model: \",model_path)\n",
    "    if 'TODBERT' in model_path:\n",
    "        model_name = \"TODBERT/TOD-BERT-JNT-V1\"\n",
    "    else:\n",
    "        model_name = \"bert-base-uncased\"\n",
    "        \n",
    "    the_model = BertClassifier(model_name,num_hidden_layer, freeze_bert=True)\n",
    "    the_model.load_state_dict(torch.load(model_path))\n",
    "    return the_model\n",
    "\n",
    "# create dataloader for the test dataset\n",
    "def get_test_dataloader(model_path,filepath):\n",
    "    if 'TODBERT' in model_path:\n",
    "        model_name = \"TODBERT/TOD-BERT-JNT-V1\"\n",
    "    else:\n",
    "        model_name = \"bert-base-uncased\"\n",
    "    folder = \"sentence\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    data = load_dataset(filepath)\n",
    "    print(filepath, len(data))\n",
    "    test_texts = list(data[:, 0])\n",
    "    test_labels = list(data[:, 1].astype(int))\n",
    "    print(\"Creating Test dataset loader..\")\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "    test_dataset = bertDataset(test_encodings, test_labels)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "    del data, test_texts, test_encodings\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "# predict label for the test dataset using the given model\n",
    "def get_predictions(model,test_loader):\n",
    "    print(\"Getting predictions..\")\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # For each batch in our test set...\n",
    "    for batch in test_loader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids = batch[\"input_ids\"]\n",
    "        b_input_mask = batch[\"attention_mask\"]\n",
    "        b_labels = batch[\"labels\"]\n",
    "        y_true.extend(b_labels.numpy())\n",
    "\n",
    "        # Compute logits\n",
    "        logits = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        probs = F.softmax(logits, dim=1).cpu().detach().numpy()\n",
    "        pred = np.argmax(probs, axis=1)\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "    d = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "    df =pd.DataFrame(data=d)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_paths = [\"Results/bert-base-uncased_2_test.torch\",\n",
    "               \"Results/bert-base-uncased_6_test.torch\",\n",
    "               \"Results/bert-base-uncased_12_test.torch\",\n",
    "               \"Results/bert-base-uncased_14_test.torch\",\n",
    "               \"Results/bert-base-uncased_18_test.torch\",\n",
    "               \"Results/TODBERT/TOD-BERT-JNT-V1_2_test.torch\",\n",
    "               \"Results/TODBERT/TOD-BERT-JNT-V1_6_test.torch\",\n",
    "               \"Results/TODBERT/TOD-BERT-JNT-V1_12_test.torch\",\n",
    "               \"Results/TODBERT/TOD-BERT-JNT-V1_14_test.torch\", \n",
    "               \"Results/TODBERT/TOD-BERT-JNT-V1_18_test.torch\",\n",
    "               \"Results/bert-base-uncased_12freeze_test.torch\",\n",
    "               \"Results/bert-base-uncased_12freeze6_test.torch\"]\n",
    "num_hidden_layers = [2,6,12,14,18,2,6,12,14,18,12,12]\n",
    "\n",
    "for i in range(len(model_paths)):\n",
    "    #load model\n",
    "    model = load_model(model_paths[i], num_hidden_layers[i])\n",
    "\n",
    "    # predict for all samples of test datset\n",
    "    for sample_num in range(20):\n",
    "        filepath = 'capstone/dataset/sentence/sample_data'+str(sample_num)+'.csv'\n",
    "        test_loader = get_test_dataloader(model_paths[i],filepath)\n",
    "        pred_df = get_predictions(model,test_loader)\n",
    "        \n",
    "        # save predictions in csv\n",
    "        if not os.path.exists(model_paths[i][:-6]):\n",
    "            os.makedirs(model_paths[i][:-6])\n",
    "        pred_df.to_csv(model_paths[i][:-6]+'/sample_result'+str(sample_num)+'.csv',index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
