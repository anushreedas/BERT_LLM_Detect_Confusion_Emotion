{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw text data and it's labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import base85 as b\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "CONF_LABELS = {\"903t1\": \"j+8KD6la:fa3?39\",\n",
    "\"668t1\": \"j.uQ81e2)Y2>evw5IK*05][W{7gX.t\",\n",
    "\"149t1\": \"v3<lf0m.u}2]%sE3Cv5B63g$A\",\n",
    "\"811t1\": \"v3&7M4dK^67:f-v\",\n",
    "\"556t1\": \"yLFa^1L<s94C}T#4UbdR6E1l#9A:L2a$>fy\",\n",
    "\"291t1\": \"yLLp+2vN{(3/}*.4OWHd5&7fE6>5058*rxbaUL5ybA-3b\",\n",
    "\"391t1\": \"y+lZ+0cFKx2[WRk4t+!06[4078QP!Db(4Au\",\n",
    "\"670t1\": \"y+kl00]TB#4a#E]\",\n",
    "\"913t1\": \"FPfUW0e*&&0W!5?3qQT4\",\n",
    "\"425t1\": \"O}{M0V.B}49n}Q4kwCu\",\n",
    "\"305t1\": \"G[aeB1&QXI2Lwwp349gd4nEvN4Czr{5BHco6?e%39Q$nfa$0u<\",\n",
    "\"622t1\": \"G[a=j2aVbu4+hgM8qckY\",\n",
    "\"194t1\": \"HfkH=1fz:p5IjUA\",\n",
    "\"833t1\": \"Hfh)>0bI^H1-aT44.@)e7TKv]bn}^!buK{C\",\n",
    "\"903t2\": \"j-9^*62+KB6[3{n97.CGdA9??fV=n7j4RSYk=sU.\",\n",
    "\"668t2\": \"j.*}m1doH/4fmcD6AG4#8c[c-8Tek/cdsmjdu2)Ah/Z}>i:a:{kH@9U\",\n",
    "\"279t2\": \"j<{971?s}C34SJA4{oK[6)yMW9wF&0dWI^}h90=O\",\n",
    "\"843t2\": \"j<(#{0{G>v2eZV)2t2i-3?T#W4{xGT5HXlN63.iy6#Z@k8F6<m8{YH#9vS2%9*X0&a)Qu%cs57Xer@pb\",\n",
    "\"149t2\": \"v4q1Q0zSVf0+8nx1.4Xj33D:o4hGGU6snND7!#1V9wex(9^P4)b*4GOej7{kgtT}BhSl*2\",\n",
    "\"811t2\": \"v4o$f2ak.O30vS03/ILx4(B(f5FL&&6r&g&caUX>gRbHgh*du>\",\n",
    "\"731t2\": \"yzTs%0r&OZ2GWym7T<)D\",\n",
    "\"177t2\": \"yzX@-15PgV27l=O5jmBXb-%}e\",\n",
    "\"556t2\": \"yL{sc0qV.e0@h#}1-BT}32f<U4Utl15rNT%7dn#28wsabacE=<bDJehb@l$@e1!PvgQ68&\",\n",
    "\"291t2\": \"yM0Yo0@SA$3uDXC6QxVPbloj{f0)p+gDmy{jtn]&k6f0I\",\n",
    "\"139t2\": \"yX6n}1gY1%a&bSZ\",\n",
    "\"998t2\": \"yX9::0nW^l2F7gr3LA(A80S0z8ShL(bKcZ{ey(z5g7=oV\",\n",
    "\"391t2\": \"y+XrD3+WqS8*9krax{4)b?jp0dUPCkfKlCWg[uXEjK<Ai\",\n",
    "\"670t2\": \"y+V5n6rSff8myE-fYt7vjEl3F\",\n",
    "\"913t2\": \"FPxCe0Gvr64UB-Y\",\n",
    "\"425t2\": \"FPf7}19>ac4Ej:77GRGK8.SpVaU+fkcIF}BdU/PNg.hSJi$RC)\",\n",
    "\"305t2\": \"G[JK00o11)1-jE52pfTl3aHri3Kvdw3{UB%6b?JC6V.g08X%xB9Gqg$b10LPbATv{dn.-Fhnq5@jOnWO\",\n",
    "\"622t2\": \"G[La<0rSUJ4lUlo6djL98}=HBdOzO]hoN(e\",\n",
    "\"194t2\": \"HfTX:0Hj5T5G8sf7@mJ$dS)Fi\",\n",
    "\"833t2\": \"HfP#V0Ypej3bm0+5l6n26C}Lz7nz!79RyLfaVQ/Eb/}x<dz)R{fnqemf!29sjcY^C\",\n",
    "\"903t3\": \"j-W$#24m&>5N=rF6A]iG7v-h]895TX8]S[^aO^tMcOW9RfI:M$jF$z&kSnVy\",\n",
    "\"668t3\": \"j-?Td5Tr#p8D!gZeibEzg/l9DhsS^UlnnDu\",\n",
    "\"279t3\": \"j>Qdv21Y4[2#Is}7e2IH95wUUcFH0=f>SpaiC*&Akd%he\",\n",
    "\"843t3\": \"j>N4f1dYQZ362sP4NRdL8!Qwlblxc>b[.ItjRx]ak6xe)lzsCW\",\n",
    "\"731t3\": \"yAn[30Hi}B5UQ<AfxAfl\",\n",
    "\"177t3\": \"yAu=H0Hi$C7PwN1bv8=}\",\n",
    "\"556t3\": \"yMR#f16L(X2n)+W3FbW-4ycGw5J{:r7cJC:7LAWe8w0<.9m#B.betS=dF>QBe9[*sf-9OAgdTk9hiG>#hYT([kF/Vql5U^z\",\n",
    "\"291t3\": \"yM.<u2H%eJ3zOsQ6&=9Gc@zJRgvWp6h]dJTi*&jJ\",\n",
    "\"139t3\": \"yX[6-0cFpE3*!u57y.z8ddYC2\",\n",
    "\"998t3\": \"yXY&L0uzwR2yM{Z3AT/p7d/o(9%A?}a@@2cdzWGsd-<s?ff7!7gw9B.g.z&lj-7f0\",\n",
    "\"391t3\": \"y=ysx1c0No2#.i&5cS.d8Ejwr9Tr^cfNU+Jkfi^8\",\n",
    "\"670t3\": \"y=vCC1yqpV6=4!<9&HjKcosFo\",\n",
    "\"913t3\": \"FQcHh0I6TJ6R=)Ycq2U5emFOV\",\n",
    "\"425t3\": \"FP>2^0NRau3ayK+66f}n9S)F*aa>=mbx1>zc?O}VedTpveIO&Se-OF=\",\n",
    "\"305t3\": \"G]j>I0pZgq2/Nuw7{s>UaZ36gdub&keSP(i\",\n",
    "\"622t3\": \"G]jTd0iVyB0>eT(4>Ow*6LOQ3ab7/0bc&Gce$%#3f^O::h}m)d\",\n",
    "\"194t3\": \"HgrK82%/mDb^/D]\",\n",
    "\"833t3\": \"Hgn&50!*+{2ZW9oa3hjc\",\n",
    "\"149t3\": \"^KD3/2gTqF7ekVK8o%oTc(aHhhqg4*kh2#p\",\n",
    "\"811t3\": \"^JYT/0X<[R3rW}.5}7zJ6E1qi8A5Kubuuv-e^0)Bhm>]^jnw<R\",}\n",
    "\n",
    "'''\n",
    "Get word token from the dataset\n",
    "'''\n",
    "def get_data(data_filepath):\n",
    "    data_arr =np.array( pd.read_csv(data_filepath, header=0, usecols=np.arange(0, 3), delimiter=',', na_values=np.nan))\n",
    "\n",
    "    # preprocess data\n",
    "    for i in range(len(data_arr[:,0])):\n",
    "        if not isinstance(data_arr[i,0],str) and math.isnan(data_arr[i,0]):\n",
    "            data_arr[i,0] = ''\n",
    "            data_arr[i,1] = 0\n",
    "        if math.isnan(data_arr[i,1]):\n",
    "            data_arr[i, 1] = 0\n",
    "\n",
    "    return data_arr\n",
    "\n",
    "\n",
    "'''\n",
    "Get label for given token using its start and end time\n",
    "'''\n",
    "def get_labels(labels,data_arr):\n",
    "    # Convert deciseconds to seconds\n",
    "    to_frame = lambda x: [(math.ceil(t / 10), v) for (t, v) in x]\n",
    "    # (time(sec) , state) pairs indicating start time for each confusion state\n",
    "    time_state = sorted(list(set([(0,0)]+to_frame(b.time16_decode(labels)[1:]))))\n",
    "    # Generate a label vector aligned to data array\n",
    "    tokenwise_label = np.array([max([s for s in time_state if t>=s[0]])[1] for t in data_arr[:,1]],dtype=np.int64)\n",
    "\n",
    "    return tokenwise_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "all_sentences = []\n",
    "def create_sentence_dataset(data_filepath):\n",
    "    # load labels\n",
    "    labels = CONF_LABELS[data_filepath.split('/')[-1][:-4] + 't' + data_filepath.split('/')[-2][-1]]\n",
    "    # load tokens\n",
    "    data_arr = get_data(data_filepath)\n",
    "    # get labels\n",
    "    tokenwise_label = get_labels(labels,data_arr)\n",
    "\n",
    "    f = tokenwise_label.reshape(-1,1)\n",
    "    all_data = np.hstack((data_arr, f))\n",
    "\n",
    "    sentences = []\n",
    "    curr_sentence = []\n",
    "    curr_sentence_labels = []\n",
    "    sentence_labels = []\n",
    "    for i in range(len(data_arr[:, 0])):\n",
    "        # check if sentence already exits in dataset\n",
    "        flag = \" \".join(curr_sentence) in all_sentences\n",
    "        # if reached end of sentence and sentence not in dataset\n",
    "        if (data_arr[i, 0] == \".\" or data_arr[i, 0] == \"?\") and not flag:\n",
    "            sentences.append(\" \".join(curr_sentence))\n",
    "            all_sentences.append(\" \".join(curr_sentence))\n",
    "            # take max of all token labels for that sentence\n",
    "            sentence_labels.append(max(curr_sentence_labels))\n",
    "            curr_sentence = []\n",
    "            curr_sentence_labels = []\n",
    "        else:\n",
    "            curr_sentence.append(data_arr[i, 0])\n",
    "            curr_sentence_labels.append(tokenwise_label[i])\n",
    "\n",
    "    # write sentence and it's label to csv file\n",
    "    with open('capstone/dataset/sentence/all/'+data_filepath.split('/')[-1][:-4]+'_dataset.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(zip(sentences, sentence_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory_name = 'capstone/dataset/T5files/data/out/txt'\n",
    "for (root,dirs,files) in os.walk(directory_name):\n",
    "    for f in files:\n",
    "        f_in = os.path.join(root,f)\n",
    "        f_out = f_in[:-4] + \"csv\"\n",
    "\n",
    "        if (len(f_in) > 5) and (f_in[-3:] == \"csv\"):\n",
    "            create_sentence_dataset(f_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Class Diatribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def class_distribution(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    plt.bar(unique, counts, 1)\n",
    "    plt.title('Class Frequency')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        data = np.array(list(reader))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = 'sentence'\n",
    "directory_name = 'capstone/dataset/'+folder+'/all'\n",
    "\n",
    "for (root,dirs,files) in os.walk(directory_name):\n",
    "    for f in files:\n",
    "        f_in = os.path.join(root,f)\n",
    "        f_out = f_in[:-4] + \"csv\"\n",
    "\n",
    "        if (len(f_in) > 5) and (f_in[-3:] == \"csv\"):\n",
    "            data = load_dataset(f_in)\n",
    "            print(f_in,len(data))\n",
    "            class_distribution(data[:,1])\n",
    "            sentences, sentence_labels = list(data[:,0]), list(data[:,1].astype(int))\n",
    "            \n",
    "            # split dataset into train, validate and test dataset\n",
    "            train_texts, test_texts, train_labels, test_labels = train_test_split(sentences, sentence_labels, test_size=.1)\n",
    "            train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.16)\n",
    "\n",
    "\n",
    "            with open('capstone/dataset/'+folder+'/sentence_train_dataset.csv', 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(zip(train_texts, train_labels))\n",
    "\n",
    "            with open('capstone/dataset/'+folder+'/sentence_val_dataset.csv', 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(zip(val_texts, val_labels))\n",
    "\n",
    "            with open('capstone/dataset/'+folder+'/sentence_test_dataset.csv', 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(zip(test_texts, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'capstone/dataset/'+folder+'/sentence_train_dataset.csv'\n",
    "data = load_dataset(filepath)\n",
    "print(filepath,len(data))\n",
    "class_distribution(data[:,1])\n",
    "            \n",
    "filepath = 'capstone/dataset/'+folder+'/sentence_val_dataset.csv'\n",
    "data = load_dataset(filepath)\n",
    "print(filepath,len(data))\n",
    "class_distribution(data[:,1])\n",
    "\n",
    "filepath = 'capstone/dataset/'+folder+'/sentence_test_dataset.csv'\n",
    "data = load_dataset(filepath)\n",
    "print(filepath,len(data))\n",
    "class_distribution(data[:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def downn_sample_dataset(data_filepath):\n",
    "    df = pd.read_csv(data_filepath, header=0, delimiter=',', names = ['sentence' , 'label'])\n",
    "\n",
    "    df_cls0 = df[df.label==0]\n",
    "    df_cls1 = df[df.label==1]\n",
    "    df_cls2 = df[df.label==2]\n",
    "    df_cls3 = df[df.label==3]\n",
    "\n",
    "    unique, counts = np.unique(df.label, return_counts=True)\n",
    "    n = min(counts)\n",
    "    print(counts,n)\n",
    "    # Downsample majority class\n",
    "    df_cls0_ds = resample(df_cls0, \n",
    "                                     replace=False,    # sample without replacement\n",
    "                                     n_samples=n,     # to match minority class\n",
    "                                     random_state=123) # reproducible results\n",
    "    df_cls1_ds = resample(df_cls1, \n",
    "                                     replace=False,    # sample without replacement\n",
    "                                     n_samples=n,     # to match minority class\n",
    "                                     random_state=123) # reproducible results\n",
    "    df_cls2_ds = resample(df_cls2, \n",
    "                                     replace=False,    # sample without replacement\n",
    "                                     n_samples=n,     # to match minority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine minority class with downsampled majority class\n",
    "    df_downsampled = pd.concat([df_cls0_ds, df_cls1_ds,df_cls2_ds,df_cls3])\n",
    "\n",
    "    # Display new class counts\n",
    "    df_downsampled.label.value_counts()\n",
    "    \n",
    "    \n",
    "    return df_downsampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_filepath = 'capstone/dataset/'+folder+'/sentence_train_dataset.csv'\n",
    "df_downsampled = downn_sample_dataset(data_filepath)\n",
    "\n",
    "df_downsampled = shuffle(df_downsampled)\n",
    "print(df_downsampled)\n",
    "df_downsampled.to_csv('capstone/dataset/'+folder+'/sentence_train_dataset_balanced.csv', sep=',', index=False,header=False)\n",
    "\n",
    "data_filepath = 'capstone/dataset/'+folder+'/sentence_val_dataset.csv'\n",
    "df_downsampled = downn_sample_dataset(data_filepath)\n",
    "\n",
    "df_downsampled = shuffle(df_downsampled)\n",
    "print(df_downsampled)\n",
    "df_downsampled.to_csv('capstone/dataset/'+folder+'/sentence_val_dataset_balanced.csv', sep=',', index=False,header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View balanced class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'capstone/dataset/'+folder+'/sentence_train_dataset_balanced.csv'\n",
    "data = load_dataset(filepath)\n",
    "print(filepath,len(data))\n",
    "class_distribution(data[:,1])\n",
    "            \n",
    "filepath = 'capstone/dataset/'+folder+'/sentence_val_dataset_balanced.csv'\n",
    "data = load_dataset(filepath)\n",
    "print(filepath,len(data))\n",
    "class_distribution(data[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
