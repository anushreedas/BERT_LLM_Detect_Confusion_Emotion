{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        data = np.array(list(reader))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'sentence'\n",
    "\n",
    "filepath = 'capstone/dataset/'+folder+'/sentence_train_dataset_balanced.csv'\n",
    "data = load_dataset(filepath)\n",
    "print(filepath,len(data))\n",
    "train_texts = list(data[:,0])\n",
    "train_labels = list(data[:,1].astype(int))\n",
    "            \n",
    "filepath = 'capstone/dataset/'+folder+'/sentence_val_dataset_balanced.csv'\n",
    "data = load_dataset(filepath)\n",
    "print(filepath,len(data))\n",
    "val_texts = list(data[:,0])\n",
    "val_labels = list(data[:,1].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data and create pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "print(train_encodings.keys())\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "class bertDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = bertDataset(train_encodings, train_labels)\n",
    "val_dataset = bertDataset(val_encodings, val_labels) \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "del data, train_texts, val_texts, train_labels, val_labels\n",
    "del train_encodings, val_encodings\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 4\n",
    "        \n",
    "        # Instantiate BERT model\n",
    "        bert_base_cased = BertModel.from_pretrained(model_name)  # Instantiate model using the trained weights\n",
    "        config = BertConfig.from_pretrained(model_name)\n",
    "        if num_hidden != 12:\n",
    "            config.num_hidden_layers = num_hidden\n",
    "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    \n",
    "num_epochs=4\n",
    "num_hidden = 12 # 2, 6, 14, 18\n",
    "model_name = \"bert-base-uncased\"\n",
    "# model_name = \"TODBERT/TOD-BERT-JNT-V1\"\n",
    "\n",
    "# Instantiate Bert Classifier\n",
    "bert_model = BertClassifier(model_name,freeze_bert=False)\n",
    "\n",
    "# Tell PyTorch to run the model on GPU\n",
    "bert_model.to(device)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(bert_model.parameters(),\n",
    "                  lr=5e-5,    # Default learning rate\n",
    "                  eps=1e-8    # Default epsilon value\n",
    "                  )\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=200, # Default value\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# load metric function\n",
    "metric_f1 = load_metric(\"f1\")\n",
    "metric_acc = load_metric(\"accuracy\")\n",
    "\n",
    "num_hidden = str(num_hidden) + 'freeze'\n",
    "print(\"training the model now...\")\n",
    "\n",
    "batch_loss, train_loss, validation_loss = [], [], []\n",
    "acc_epoch = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # put model in train mode\n",
    "    bert_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f'=========Running {epoch} of {num_epochs}=========')\n",
    "    print(\"Running training batch...\")\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'--- Processing batch {i} ----')\n",
    "            \n",
    "        # Zero out any previously calculated gradients\n",
    "        bert_model.zero_grad()\n",
    "\n",
    "        b_input_ids = batch[\"input_ids\"].to(device)\n",
    "        b_input_mask = batch[\"attention_mask\"].to(device)\n",
    "        b_labels = batch[\"labels\"].to(device)\n",
    "        outputs = bert_model(b_input_ids, attention_mask=b_input_mask)#,labels=b_labels)\n",
    "\n",
    "        # Compute loss and accumulate the loss values\n",
    "        loss = loss_fn(outputs, b_labels)\n",
    "        batch_loss.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "#         for i in range(len(b_input_ids)):\n",
    "#             print(tokenizer.convert_ids_to_tokens(b_input_ids[i],skip_special_tokens = False),b_labels[i])\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and the learning rate\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_loss.append(avg_train_loss)\n",
    "    print(f\"\\nAverage train loss: {avg_train_loss}\")\n",
    "    \n",
    "    # put model in evalauation model\n",
    "    bert_model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    print(\"Running validation batch...\")\n",
    "    for i, batch in enumerate(eval_loader):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'--- Processing {i} ----')\n",
    "        b_input_ids = batch[\"input_ids\"].to(device)\n",
    "        b_input_mask = batch[\"attention_mask\"].to(device)\n",
    "        b_labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # get predictions for eval dataset\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "        loss = loss_fn(outputs, b_labels)\n",
    "        eval_loss += loss    \n",
    "        \n",
    "        predictions.extend(torch.argmax(outputs, dim=1).flatten())\n",
    "        true_labels.extend(b_labels)\n",
    "    \n",
    "    eval_loss = eval_loss / len(eval_loader)\n",
    "    validation_loss.append(eval_loss)\n",
    "    print(f\"Validation loss: {eval_loss}\")\n",
    "\n",
    "    # calculate f1 score and accuracy\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=true_labels,average=None)\n",
    "    print(f'Validation F1 score: {f1[\"f1\"]}')\n",
    "    acc = metric_acc.compute(predictions=predictions, references=true_labels)\n",
    "    print(f'\\nValidation Accuracy: {acc[\"accuracy\"]}')\n",
    "    acc_epoch.append(acc[\"accuracy\"])\n",
    "    \n",
    "    # plot losses and accuracy\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    \n",
    "    ax1.plot(train_loss)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('loss value')\n",
    "    \n",
    "    ax3.plot(acc_epoch)\n",
    "    ax3.set_title('Accuracy')\n",
    "    ax3.set_xlabel('epoch')\n",
    "    ax3.set_ylabel('acc value')\n",
    "\n",
    "    ax2.plot(validation_loss)\n",
    "    ax2.set_title('Validation Loss')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('loss value')\n",
    "    \n",
    "    plt.savefig(\"Results/\"+model_name+\"_\"+num_hidden+\"_\"+\"results.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    f = open(\"Results/\"+model_name+\"_\"+num_hidden+\"_results.txt\", \"a\")\n",
    "    f.write(f\"\\nAverage train loss: {avg_train_loss}\")\n",
    "    f.write(f\"Validation loss: {eval_loss}\")\n",
    "    f.write(f'Validation F1 score: {f1[\"f1\"]}')\n",
    "    f.write(f'\\nValidation Accuracy: {acc[\"accuracy\"]}')\n",
    "    f.close()\n",
    "    \n",
    "# pront time taken to train model\n",
    "end =time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:0>2}\".format(int(hours),int(minutes),int(seconds)))\n",
    "\n",
    "# save model\n",
    "torch.save(bert_model.state_dict(), \"Results/\"+model_name+\"_\"+num_hidden+\"_test.torch\")\n",
    "\n",
    "f = open(\"Results/\"+model_name+\"_\"+num_hidden+\"_results.txt\", \"a\")\n",
    "f.write(\"\\n Time elapsed {:0>2}:{:0>2}:{:0>2}\".format(int(hours),int(minutes),int(seconds)))\n",
    "f.close()\n",
    "print(\"{:0>2}:{:0>2}:{:0>2}\".format(int(hours),int(minutes),int(seconds)))\n",
    "\n",
    "print(\"Done\",model_name+\"_\"+num_hidden+\"_results\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
